#task 1: Jaccard based LSH
#use yelp_train.csv
#0 or 1 based on whether a use rated a business
#submission: spark-submit task1.py <input_file_name> <output_file_name>
#other criteria: n >= 0.99 and recall >= 0.97, <100s

import sys, csv, time, random
from pyspark import SparkConf, SparkContext 
from itertools import combinations
from functools import partial

#setting up hashing params
#prompt says >= 0.5
# choosing to use f(x) = ((ax + b) % p) % m
#You should carefully select a good combination of b and r in your implementation (b>1 and r>1).
p = 1299709
m = None
jaccard_similarity_threshold = 0.5
b = 20
r = 3
n = b*r
s = 42

#taken from past assignmnt, modified to fit this one 
def check_inputs():
    #check for 3 inputs (py, input, output)
    if len(sys.argv) != 3:
        print("Usage: spark-submit task1.py <input_file_name> <output_file_name>", file=sys.stderr)
        sys.exit(1)

    #dont need to check other bits for anything
    return sys.argv[1], sys.argv[2] 

#func for reading csv to rdd 
def csv_to_rdd(rows):
    for row in csv.reader(rows):

        #skip if not row or wrong row size
        if not row or len(row) < 2:
            continue

        #account for headers
        if row[0] == "user_id" or row[1] == "business_id":
            continue
        
        #clean the exact data rows and columns, can ignore the stars because essentially having user and business exist = stars = 1
        user_id = row[0].strip()
        business_id  = row[1].strip()

        if user_id and business_id:
            yield (user_id, business_id)

#func to use func above across partitions and to avoid double counts
def read_user_business_rdd(sc, input_path):
    rdd = sc.textFile(input_path)
    
    #reads csv for each partition
    pairs = rdd.mapPartitions(csv_to_rdd)
    
    #avoid double count
    return pairs.distinct()

#buidling the matrix for applying jaccard to
#technically not building a full on 0/1s matrix because it isnt efficient (just sets)
def build_matrix(user_business_rdd, user_indices):
    #taking our original rdd and flipping, and checking for valid users (indexed at >=0)
    business_valid_user_rdd = user_business_rdd.map(lambda u: (u[1], user_indices.get(u[0], -1))).filter(lambda kv: kv[1]>=0)

    #syntax is aggregateByKey(zeroValue, seqFunc, combFunc) reminder: base value, merge a v into u, combine two u's into a single u
    business_userset_rdd = business_valid_user_rdd.aggregateByKey(set(), lambda s, u: (s.add(u) or s), lambda s1, s2: (s1.update(s2) or s1))

    #should return liike: rdd of businerss id : set of user indexes
    return business_userset_rdd

#turns user ids into index numbers (added this in to help speed, faster to hash this than a bunch of individiaul very unique IDs)
def build_user_index(user_business_rdd):
    #should take just the user id and create distinct indexing for each one
    return dict(user_business_rdd.map(lambda x: x[0]).distinct().zipWithIndex().collect())

#minhash and lsh
#rule of thumb: use  prime slightly larger than 'universe size' so in our case, large prime > possible ids
#better to use larger modulus
#random seed: 42 (meaning of life one)
#building: f(x) = ((ax + b) % p) % m
def build_hash_parameters(n_hash=n, prime=p, seed=s):
    #set base params
    parameters = set()
    rng = random.Random(seed)

    #until we satifsy n hashes in the parameters (generated by a,b pairs)
    while len(parameters) < n_hash:
        #the slope and offset of the hash function ax+b
        a = rng.randrange(1, prime)
        b = rng.randrange(0, prime)

        #adds pairs of a b to params
        parameters.add((a, b))
    #sorted before return
    return sorted(parameters)
    
def build_minhash(users, hash_parameters, prime=p, m_bins=m):
    #put the set of a/b from hash parameters into an array (faster to iterate through)
    array_a = [ab[0] for ab in hash_parameters]
    array_b = [ab[1] for ab in hash_parameters]
    #use to iterate through each user id as an index (to hash each user)
    user_index = iter(users)
    first_x = next(user_index)     
    hash_count = len(hash_parameters)
    #stores the hashes
    signature = [0]*hash_count  
    #iterate through count of parameters
    for i in range(hash_count):
        #hash formula written out
        hashes = (((array_a[i]*first_x + array_b[i]) % prime) % m_bins)
        #signature will now store all hashes
        signature[i] = hashes

    #go through the rest of the x values into the hash formula, updates min hahs
    for x in user_index:
        #copy of above, but now use each x instead of the first 
        for i in range(hash_count):
            hashes = (((array_a[i]*x + array_b[i]) % prime) % m_bins)
            #find the smallest hash to be the "minhash"
            if hashes < signature[i]:
                signature[i] = hashes

    return signature

#build LSH now 
#takes rdd of business,signatures
def lsh(business_signature_rdd, bands=b, rows=r):


    band_key_values = business_signature_rdd.flatMap(
        #item = business id, signature
        lambda item: (
            #haha i thought it would be funny to use bandai like band_i but bandai namco
            #new key is: (bandai, band_tuple) and value is business id
            #should group them into sam bands and the business ids in that band
            ((bandai, tuple(item[1][bandai*rows:(bandai+1)*rows])), item[0])
            for bandai in range(bands)
        )
    )

    #groups the above by key (bandai, band_tuple)
    buckets = band_key_values.groupByKey()

    #takes buckets and creates candidate pairs
    #takes the (bandai, band_tuple), aggregated on those
    #then should 
    candidates = (buckets
                  #flatmap and gets all of the '2' combinations (basically checking all the pairs we have)
                  .flatMap(lambda kv: combinations(sorted(set(kv[1])), 2))
                  #order the pairs in smaller, bigger 
                  .map(lambda p: p if p[0] < p[1] else (p[1], p[0]))
                  #make sure to remove dupes
                  .distinct())
    
    #should return final candidates list
    return candidates

#calculates jaccard similarity score, 1-this for distance
def jaccard_similarity(candidates, business_users_mapping):
    a, b = candidates
    candidate_a = business_users_mapping.get(a, set())
    candidate_b = business_users_mapping.get(b, set())

    if not candidate_a and not candidate_b:
        return 0.0
    
    intersection = float(len(candidate_a.intersection(candidate_b)))
    union = float(len(candidate_a) + len(candidate_b) - intersection)
    return float(intersection/union) 

#just in case we need distance
def jaccard_distance(candidates, business_users_mapping):
    return 1.0-jaccard_similarity(candidates, business_users_mapping)

#output formatting 
#format of output needs to be: csv file
#business_id_1, business_id_2, similarity and must be sorted in lexicographical order
#writes the resulting rdd from all the calcs into a file
def make_output(results_rdd, output_path):
    
    #rows are the results sorted in ids then the jaccard similarity
    rows = (results_rdd
            .sortBy(lambda t: (t[0], t[1]))
            .collect())

    with open(output_path, "w") as f:
        f.write("business_id_1,business_id_2,similarity\n")
        for bid1, bid2, sim in rows:
            #write all 3 things
            f.write(f"{bid1},{bid2},{sim}\n")

#taken from last assignment
#timers for output in main (time tracking for parts)
#timer start
def start_timer():
    return time.perf_counter()

#timer end and print duration oftime 
def end_timer(t0, label="Duration"):
    secs = int(time.perf_counter() - t0)
    #adds to show Duration: whatever seconds
    print(f"{label}: {secs}")
    return secs

#main to put everything together
def main():
    input_path, output_path = check_inputs()

    #build spark configu
    conf = SparkConf().setAppName("task1")
    sc = SparkContext(conf=conf)
    #less clutter when running code
    sc.setLogLevel("WARN")

    #start timer (tracking start)
    t0 = start_timer()

    #initial rdd from the input
    user_business_rdd = read_user_business_rdd(sc, input_path)

    #get user index
    user_to_index = build_user_index(user_business_rdd)

    #edit the global m initially written
    global m
    m = len(user_to_index)

    #matrix of users and which businesses they rated
    business_users_rdd = build_matrix(user_business_rdd, user_to_index).persist()

    #use prior func to get our hashing parameters to use for minhash, lsh, etc. 
    hash_parameters = build_hash_parameters(n_hash=n, prime=p, seed=s)

    #minhash on the business+signatures matrix
    business_signatures_rdd = business_users_rdd.map(
    lambda kv: (kv[0], build_minhash(kv[1], hash_parameters, prime=p, m_bins=m))
    ).persist()
    
    #lsh to get candidates
    candidates_rdd = lsh(business_signatures_rdd, bands=b, rows=r)

    #get dict of this, and essentially adding in jaccard similarity values into it which is why we need to recall this again
    business_users_map = dict(business_users_rdd.collect())
    bc_business_users_map = sc.broadcast(business_users_map)

    #write calculations to results_rdd
    results_rdd = (candidates_rdd
                   .map(lambda pair: (pair[0], pair[1],
                                      jaccard_similarity(pair, bc_business_users_map.value)))
                   .filter(lambda t: t[2] >= jaccard_similarity_threshold))

    #write to output file
    make_output(results_rdd, output_path)

    #end timer and print duration, show time taken
    end_timer(t0, label="Duration")

    #stop spark
    sc.stop()


if __name__ == "__main__":
    main()